# Opinions Aren’t Evidence. Show Your Damn Receipts.

The internet has two speeds right now:

1. **Instant takes**
2. **Instant regret**

Somebody posts a spicy claim. Everyone quote-tweets it like it’s gospel. Twelve hours later we discover the “source” was a screenshot of a screenshot posted by a guy named CryptoWarlord69 with an anime avatar and no last name.

Incredible system. No notes.

So here’s today’s Dolphin Notes sermon: if you’re making factual claims in public, bring receipts. If you *can’t* verify the claim, call it opinion, uncertainty, or analysis — not truth from the mountain.

Not because we’re trying to be boring hall monitors.
Because false confidence is expensive.

## Confidence theater is not competence

A confident tone is cheap. Evidence is work.

And online, people constantly confuse the two:

- “Sounds certain” becomes “must be true.”
- “Went viral” becomes “must be important.”
- “Has a chart” becomes “must be science.”

Nope.

A chart can still be garbage. A thread can still be fiction. A confident narrator can still be wrong as hell.

### Receipt #1: Misinformation spreads faster than corrections

This isn’t just vibes. Research has repeatedly shown that falsehood can spread quickly and widely online, while corrections lag behind.

- MIT study in *Science* (2018): false news reached more people and spread faster on Twitter than truthful news in the sampled data: <https://www.science.org/doi/10.1126/science.aap9559>

You don’t have to read every equation to understand the operational takeaway:
**bad info has a head start**.

So when you publish a claim without verification, you’re not “joining the conversation.” You might be greasing the slide.

## “I heard” is not a source

Let’s set a stupidly practical standard.

If you make a factual claim, at minimum you should be able to answer:

1. **Where did this come from?** (primary source if possible)
2. **When was it published or updated?**
3. **What are the limits?** (sample size, context, caveats)
4. **Can another person independently verify it?**

If your answer is “uh, I saw it in a Discord,” that’s not a source. That’s gossip with better typography.

## Primary beats derivative (most of the time)

If a headline says “New study proves X,” try to find the actual study.
If a blog says “company Y confirmed Z,” go find the company statement or filing.
If a screenshot claims a policy changed, find the policy page.

### Receipt #2: NIST explicitly emphasizes documentation, transparency, and traceability in AI risk management

- NIST AI Risk Management Framework 1.0: <https://www.nist.gov/itl/ai-risk-management-framework>
- PDF: <https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf>

You’ll notice this theme everywhere in serious work: **document assumptions, track provenance, make claims auditable**.

That’s not bureaucracy cosplay. That’s how you avoid building nonsense on top of nonsense.

## Yes, opinion is still valuable

I’m not saying every post needs twelve citations and a peer-review board in your comments.

Hot takes are fine — if you label them honestly.

Try this framing:

- **Fact claim:** “This benchmark used dataset X and excluded Y.” → bring source.
- **Interpretation:** “That exclusion probably inflates performance.” → explain reasoning.
- **Opinion:** “I don’t trust benchmark culture without methods.” → totally fair, just own it.

The problem isn’t opinions.
The problem is smuggling opinions over the border dressed as facts.

## The “receipt ladder” you can actually use

When you’re writing quickly, use this simple ladder:

- **Gold:** Primary documents (papers, official docs, filings, raw data)
- **Silver:** Reputable reporting that links primary sources
- **Bronze:** Secondary summaries with clear citations
- **Tin foil:** Anonymous screenshot + “trust me bro”

If you’re standing on tin foil, don’t sprint. Phrase cautiously or skip the claim.

## Why this matters for builders (not just journalists)

Engineers, founders, students, researchers — same rules.

Bad assumptions make expensive bugs.

You copy a Reddit config tweak without checking official docs? Enjoy your production fire.
You quote an AI benchmark from a screenshot and choose tooling around it? Congrats, you just architected a stack around potentially fake numbers.

### Receipt #3: Reproducibility and documentation are core quality norms in software and research culture

- ACM Artifact Review and Badging (reproducibility incentives): <https://www.acm.org/publications/policies/artifact-review-and-badging-current>
- Nature on reproducibility concerns in science (overview): <https://www.nature.com/articles/533452a>

Different domains, same lesson:
if nobody can reproduce or verify the claim, treat it as provisional at best.

## A tiny template for honest writing

If you publish technical takes, steal this mini footer format:

- **What I know:** (facts with links)
- **What I infer:** (reasoned interpretation)
- **What I don’t know yet:** (uncertainties)

That one habit instantly upgrades trust.
You look less like a hype merchant and more like a responsible adult with internet access.

## “But speed matters”

True. Speed matters.

But bad information has carrying costs:

- wrong decisions,
- wasted engineering time,
- reputational damage,
- and the slow death of audience trust.

Trust is like your phone battery on an old Android in winter: easy to drain, annoyingly slow to recover.

So yeah, post fast when needed. But not faster than your ability to check whether your own sentence is bullshit.

## Final take

If it’s a fact, cite it.
If it’s an inference, explain it.
If it’s a vibe, label it.

That’s it.

You don’t need to become a librarian monk.
Just stop treating confidence as a substitute for evidence.

Because “I said it loudly” is not a methodology.
It’s just karaoke for misinformation.
